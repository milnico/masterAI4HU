{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lezione8_backpropagation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/milnico/masterAI4HU/blob/main/Lezione8_backpropagation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "id": "K6k4BBO--RIz",
        "outputId": "cda46796-a3df-49f4-e726-14c55c67611d"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1/(1+np.exp(-x))\n",
        "\n",
        "def sigmoid_der(x):\n",
        "    return x*(1-x)\n",
        "\n",
        "def step_func(z):\n",
        "        return np.where(z<0.5,0,1)\n",
        "\n",
        "class NN:\n",
        "    def __init__(self,inputs):\n",
        "        self.inputs = inputs\n",
        "        self.h =4#len(inputs)\n",
        "        self.li = len(inputs[0])\n",
        "        self.wi = (2*np.random.random((self.li,self.h)))-1\n",
        "        self.wh = (2*np.random.random((self.h,1)))-1\n",
        "        self.lr = 1\n",
        "        \n",
        "    def predict(self,inp):\n",
        "        s1=sigmoid(np.dot(inp,self.wi))\n",
        "        s2=sigmoid(np.dot(s1,self.wh))\n",
        "        return s2\n",
        "\n",
        "\n",
        "    def train(self,inputs,outputs,it):\n",
        "        for i in range(it):\n",
        "            l0= inputs\n",
        "            print(np.shape(l0))\n",
        "            l1=sigmoid(np.dot(l0,self.wi))\n",
        "            print(np.shape(l1))\n",
        "            l2 = sigmoid(np.dot(l1,self.wh))\n",
        "            print(np.shape(l2))\n",
        "            input(\"eee\")\n",
        "            l2_err=outputs-l2\n",
        "            #print l2_err\n",
        "            l2_delta=np.multiply(l2_err,sigmoid_der(l2))\n",
        "\n",
        "            l1_err = np.dot(l2_delta,self.wh.T)\n",
        "            l1_delta = np.multiply(l1_err,sigmoid_der(l1))\n",
        "\n",
        "            self.wh += self.lr*np.dot(l1.T,l2_delta)\n",
        "            self.wi += self.lr*np.dot(l0.T,l1_delta)\n",
        "\n",
        "inputs = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "outputs = np.array([[0],[1],[1],[0]])\n",
        "net = NN(inputs)\n",
        "print(\"Before\")\n",
        "print(net.predict(inputs))\n",
        "net.train(inputs,outputs,10000)\n",
        "print(\"After \")\n",
        "print(net.predict(inputs))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before\n",
            "[[0.36430304]\n",
            " [0.42102258]\n",
            " [0.39072361]\n",
            " [0.44111349]]\n",
            "(4, 2)\n",
            "(4, 4)\n",
            "(4, 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    582\u001b[0m         \"\"\"\n\u001b[0;32m--> 583\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-94fb66f7004c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Before\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"After \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-94fb66f7004c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, inputs, outputs, it)\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0ml2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"eee\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0ml2_err\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0ml2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;31m#print l2_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ox-EtN_wH6QO"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from tqdm import tqdm\n",
        "#train_inputs, train_targets = datasets.make_blobs(n_samples=150,n_features=2,centers=2,cluster_std=1.05,random_state=2)\n",
        "train_inputs, train_targets = datasets.make_moons(n_samples=500, noise=.05)\n",
        "#Plotting\n",
        "fig = plt.figure(figsize=(6,4))\n",
        "plt.plot(train_inputs[:, 0][train_targets == 0], train_inputs[:, 1][train_targets == 0], 'r^')\n",
        "plt.plot(train_inputs[:, 0][train_targets == 1], train_inputs[:, 1][train_targets == 1], 'bs')\n",
        "plt.xlabel(\"feature 1\")\n",
        "plt.ylabel(\"feature 2\")\n",
        "plt.title('Random Classification Data with 2 classes')\n",
        "plt.show()\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1/(1+np.exp(-x))\n",
        "\n",
        "def sigmoid_der(x):\n",
        "    return x*(1-x)\n",
        "\n",
        "\n",
        "class NN:\n",
        "    def __init__(self,inputs):\n",
        "        self.inputs = inputs\n",
        "        self.h =10#len(inputs)\n",
        "        self.li = len(inputs[0])\n",
        "        self.wi = (2*np.random.random((self.li,self.h)))-1\n",
        "        self.wh = (2*np.random.random((self.h,1)))-1\n",
        "        self.lr = 0.05\n",
        "        #self.batch = int(len(inputs)/150)\n",
        "        #print(self.batch)\n",
        "\n",
        "    def predict(self,inp):\n",
        "        \n",
        "        s1=sigmoid(np.dot(inp,self.wi))\n",
        "        s2=sigmoid(np.dot(s1,self.wh))\n",
        "        return s2\n",
        "\n",
        "\n",
        "    def train(self,outputs,epochs): \n",
        "        for epoch in tqdm(range(epochs)):\n",
        "          #print(epoch)\n",
        "          # looping for every example.\n",
        "          loss = []\n",
        "          #num = len(self.inputs)#int(len(self.inputs)/self.batch)\n",
        "          for i,x_i in enumerate(self.inputs):\n",
        "              \n",
        "             \n",
        "              l0 = x_i.reshape((1,3))\n",
        "              #l0 = self.inputs[self.batch*i:self.batch*(i+1)].reshape((len(self.inputs[self.batch*i:self.batch*(i+1)]),3))\n",
        "              #print(l0,np.shape(l0))\n",
        "              \n",
        "              l1=sigmoid(np.dot(l0,self.wi))\n",
        "              #print(l1,np.shape(l1))\n",
        "              l2 = sigmoid(np.dot(l1,self.wh))\n",
        "              #print(l2,np.shape(l2))\n",
        "              \n",
        "              l2_err=(outputs[i]-l2)**2\n",
        "              loss.append(np.mean(l2_err))\n",
        "              \n",
        "              out_delta = np.multiply(l2_err,(outputs[i]-l2)*2) \n",
        "               \n",
        "              l2_delta=np.multiply(out_delta,sigmoid_der(l2))\n",
        "              #print(l2_delta,np.shape(l2_delta))\n",
        "              l1_err = np.dot(l2_delta,self.wh.T)\n",
        "              #print(l1_err,np.shape(l1_err))\n",
        "              l1_delta = np.multiply(l1_err,sigmoid_der(l1))\n",
        "\n",
        "              self.wh += self.lr*np.dot(l1.T,l2_delta)\n",
        "              self.wi += self.lr*np.dot(l0.T,l1_delta)\n",
        "          \n",
        "          #print(np.mean(loss))\n",
        "          \n",
        "\n",
        "train_inputs = np.insert(train_inputs,2,1,axis=1)\n",
        "net = NN(train_inputs)\n",
        "net.train(train_targets,500)\n",
        "\n",
        "pred = net.predict(train_inputs)\n",
        "pred = np.where(pred<0.5,0,1)\n",
        "print(pred.reshape(len(pred)))\n",
        "print(train_targets.reshape(len(pred)))\n",
        "print(np.shape(pred))\n",
        "print(np.shape(train_targets))\n",
        "#Plotting\n",
        "fig = plt.figure(figsize=(6,4))\n",
        "plt.plot(train_inputs[:, 0][pred.reshape(len(pred)) == 0], train_inputs[:, 1][pred.reshape(len(pred)) == 0], 'r^')\n",
        "plt.plot(train_inputs[:, 0][pred.reshape(len(pred)) == 1], train_inputs[:, 1][pred.reshape(len(pred)) == 1], 'bs')\n",
        "plt.xlabel(\"feature 1\")\n",
        "plt.ylabel(\"feature 2\")\n",
        "plt.title('Random Classification Data with 2 classes')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnhkJo85IQuY"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from tqdm import tqdm\n",
        "#train_inputs, train_targets = datasets.make_blobs(n_samples=150,n_features=2,centers=2,cluster_std=1.05,random_state=2)\n",
        "train_inputs, train_targets = datasets.make_moons(n_samples=1500, noise=.05)\n",
        "#Plotting\n",
        "fig = plt.figure(figsize=(6,4))\n",
        "plt.plot(train_inputs[:, 0][train_targets == 0], train_inputs[:, 1][train_targets == 0], 'r^')\n",
        "plt.plot(train_inputs[:, 0][train_targets == 1], train_inputs[:, 1][train_targets == 1], 'bs')\n",
        "plt.xlabel(\"feature 1\")\n",
        "plt.ylabel(\"feature 2\")\n",
        "plt.title('Random Classification Data with 2 classes')\n",
        "plt.show()\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1/(1+np.exp(-x))\n",
        "\n",
        "def sigmoid_der(x):\n",
        "    return x*(1-x)\n",
        "\n",
        "\n",
        "class NN:\n",
        "    def __init__(self,inputs):\n",
        "        self.inputs = inputs\n",
        "        self.h =10#len(inputs)\n",
        "        self.li = len(inputs[0])\n",
        "        self.wi = (2*np.random.random((self.li,self.h)))-1\n",
        "        self.wh = (2*np.random.random((self.h,1)))-1\n",
        "        self.lr = 0.05\n",
        "        self.batch = int(len(inputs)/10)\n",
        "        #print(self.batch)\n",
        "\n",
        "    def predict(self,inp):\n",
        "        \n",
        "        s1=sigmoid(np.dot(inp,self.wi))\n",
        "        s2=sigmoid(np.dot(s1,self.wh))\n",
        "        return s2\n",
        "\n",
        "\n",
        "    def train(self,outputs,epochs): \n",
        "        for epoch in range(epochs):\n",
        "          #print(epoch)\n",
        "          # looping for every example.\n",
        "          loss = []\n",
        "          num = int(len(self.inputs)/self.batch)\n",
        "          for i in range(num):\n",
        "              \n",
        "             \n",
        "              #l0 = x_i.reshape((1,3))\n",
        "              l0 = self.inputs[self.batch*i:self.batch*(i+1)].reshape((len(self.inputs[self.batch*i:self.batch*(i+1)]),3))\n",
        "              #print(l0,np.shape(l0))\n",
        "              \n",
        "              l1=sigmoid(np.dot(l0,self.wi))\n",
        "              #print(l1,np.shape(l1))\n",
        "              l2 = sigmoid(np.dot(l1,self.wh))\n",
        "              #print(l2,np.shape(l2))\n",
        "              l2_err=(np.expand_dims(outputs[self.batch*i:self.batch*(i+1)],axis=1)-l2)**2\n",
        "              #l2_err=(outputs[i]-l2)**2\n",
        "              loss.append(np.mean(l2_err))\n",
        "              \n",
        "              #out_delta = np.multiply(l2_err,(outputs[i]-l2)*2) \n",
        "              out_delta = np.multiply(l2_err,(np.expand_dims(outputs[self.batch*i:self.batch*(i+1)],axis=1)-l2)*2) \n",
        "              l2_delta=np.multiply(out_delta,sigmoid_der(l2))\n",
        "              #print(l2_delta,np.shape(l2_delta))\n",
        "              l1_err = np.dot(l2_delta,self.wh.T)\n",
        "              #print(l1_err,np.shape(l1_err))\n",
        "              l1_delta = np.multiply(l1_err,sigmoid_der(l1))\n",
        "\n",
        "              self.wh += self.lr*np.dot(l1.T,l2_delta)\n",
        "              self.wi += self.lr*np.dot(l0.T,l1_delta)\n",
        "          \n",
        "          #print(np.mean(loss))\n",
        "          \n",
        "\n",
        "train_inputs = np.insert(train_inputs,2,1,axis=1)\n",
        "net = NN(train_inputs)\n",
        "net.train(train_targets,500)\n",
        "\n",
        "pred = net.predict(train_inputs)\n",
        "pred = np.where(pred<0.5,0,1)\n",
        "print(pred.reshape(len(pred)))\n",
        "print(train_targets.reshape(len(pred)))\n",
        "print(np.shape(pred))\n",
        "print(np.shape(train_targets))\n",
        "#Plotting\n",
        "fig = plt.figure(figsize=(6,4))\n",
        "plt.plot(train_inputs[:, 0][pred.reshape(len(pred)) == 0], train_inputs[:, 1][pred.reshape(len(pred)) == 0], 'r^')\n",
        "plt.plot(train_inputs[:, 0][pred.reshape(len(pred)) == 1], train_inputs[:, 1][pred.reshape(len(pred)) == 1], 'bs')\n",
        "plt.xlabel(\"feature 1\")\n",
        "plt.ylabel(\"feature 2\")\n",
        "plt.title('Random Classification Data with 2 classes')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ixZcaXDfJ237",
        "outputId": "a469a18c-ba55-484c-b87b-1efe901ee9d3"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from tqdm import tqdm\n",
        "\n",
        "train_inputs, train_targets = datasets.load_breast_cancer(return_X_y=True)\n",
        "\n",
        "\n",
        "def sigmoid(x):\n",
        "\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "\n",
        "def sigmoid_der(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "\n",
        "class NN:\n",
        "    def __init__(self, inputs):\n",
        "        self.inputs = inputs\n",
        "        self.h = 50  # len(inputs)\n",
        "        self.li = len(inputs[0])\n",
        "        self.wi = (2 * np.random.random((self.li, self.h))) - 1\n",
        "        self.wh = (2 * np.random.random((self.h, 1))) - 1\n",
        "        self.lr = 0.05\n",
        "        #self.batch = int(len(inputs) / len(inputs))\n",
        "        # print(self.batch)\n",
        "\n",
        "    def predict(self, inp):\n",
        "\n",
        "        s1 = sigmoid(np.dot(inp, self.wi))\n",
        "        s2 = sigmoid(np.dot(s1, self.wh))\n",
        "        return s2\n",
        "\n",
        "    def train(self, outputs, epochs):\n",
        "        for epoch in range(epochs):\n",
        "            # print(epoch)\n",
        "            # looping for every example.\n",
        "            loss = []\n",
        "            \n",
        "            for i,x_i in enumerate(self.inputs):\n",
        "              \n",
        "             \n",
        "              l0 = x_i.reshape((1,self.li))\n",
        "              #l0 = self.inputs[self.batch*i:self.batch*(i+1)].reshape((len(self.inputs[self.batch*i:self.batch*(i+1)]),3))\n",
        "              #print(l0,np.shape(l0))\n",
        "              \n",
        "              l1=sigmoid(np.dot(l0,self.wi))\n",
        "              #print(l1,np.shape(l1))\n",
        "              l2 = sigmoid(np.dot(l1,self.wh))\n",
        "              #print(l2,np.shape(l2))\n",
        "              \n",
        "              l2_err=(outputs[i]-l2)**2\n",
        "              loss.append(np.mean(l2_err))\n",
        "              \n",
        "              out_delta = np.multiply(l2_err,(outputs[i]-l2)*2) \n",
        "               \n",
        "              l2_delta=np.multiply(out_delta,sigmoid_der(l2))\n",
        "              #print(l2_delta,np.shape(l2_delta))\n",
        "              l1_err = np.dot(l2_delta,self.wh.T)\n",
        "              #print(l1_err,np.shape(l1_err))\n",
        "              l1_delta = np.multiply(l1_err,sigmoid_der(l1))\n",
        "\n",
        "              self.wh += self.lr*np.dot(l1.T,l2_delta)\n",
        "              self.wi += self.lr*np.dot(l0.T,l1_delta)\n",
        "\n",
        "            print(np.mean(loss))\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "train_inputs = sc.fit_transform(train_inputs)\n",
        "\n",
        "\n",
        "print(np.shape(train_targets))\n",
        "net = NN(train_inputs)\n",
        "net.train(train_targets, 500)\n",
        "\n",
        "pred = net.predict(train_inputs)\n",
        "pred = np.where(pred < 0.5, 0, 1)\n",
        "print(pred)\n",
        "print(train_targets)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(569,)\n",
            "0.08896422331085294\n",
            "0.057879161201523636\n",
            "0.050075067303680706\n",
            "0.04618661948862251\n",
            "0.04361920827170322\n",
            "0.04169215246146328\n",
            "0.04014746909228879\n",
            "0.03886175048986861\n",
            "0.03776558495087955\n",
            "0.036815187168270605\n",
            "0.03598060578276384\n",
            "0.03524014987722807\n",
            "0.03457746364020122\n",
            "0.03397984951350112\n",
            "0.03343723361907365\n",
            "0.03294148815513263\n",
            "0.03248596616457596\n",
            "0.03206517023898295\n",
            "0.03167450987720969\n",
            "0.03131011985975264\n",
            "0.03096872193145299\n",
            "0.030647517974379923\n",
            "0.030344106512485765\n",
            "0.0300564167594135\n",
            "0.02978265601070848\n",
            "0.02952126728103576\n",
            "0.029270894866907055\n",
            "0.0290303560808363\n",
            "0.028798617820309083\n",
            "0.028574776947843564\n",
            "0.02835804369573688\n",
            "0.028147727490633243\n",
            "0.027943224732649343\n",
            "0.027744008171319792\n",
            "0.027549617603306244\n",
            "0.02735965168007392\n",
            "0.027173760661756537\n",
            "0.02699163998950768\n",
            "0.026813024575418704\n",
            "0.02663768372871685\n",
            "0.026465416651187602\n",
            "0.026296048444996455\n",
            "0.026129426583440182\n",
            "0.0259654178005036\n",
            "0.025803905359106517\n",
            "0.02564478666107286\n",
            "0.025487971164495974\n",
            "0.025333378576530723\n",
            "0.02518093729186095\n",
            "0.025030583049251896\n",
            "0.0248822577807273\n",
            "0.02473590863002729\n",
            "0.024591487119081407\n",
            "0.024448948443262102\n",
            "0.024308250878140068\n",
            "0.024169355282321715\n",
            "0.024032224682700918\n",
            "0.02389682393007916\n",
            "0.023763119414604707\n",
            "0.023631078831837855\n",
            "0.023500670991476232\n",
            "0.023371865661867566\n",
            "0.02324463344441139\n",
            "0.02311894567280541\n",
            "0.02299477433284422\n",
            "0.022872091999129353\n",
            "0.02275087178561593\n",
            "0.022631087307409592\n",
            "0.022512712651643473\n",
            "0.022395722355625494\n",
            "0.022280091390749313\n",
            "0.022165795150920455\n",
            "0.02205280944446958\n",
            "0.021941110488707994\n",
            "0.021830674906435924\n",
            "0.021721479723845038\n",
            "0.021613502369363942\n",
            "0.021506720673088304\n",
            "0.021401112866510273\n",
            "0.021296657582326156\n",
            "0.021193333854151897\n",
            "0.021091121116019265\n",
            "0.020989999201558782\n",
            "0.020889948342805914\n",
            "0.020790949168587786\n",
            "0.02069298270246766\n",
            "0.020596030360238098\n",
            "0.020500073946965663\n",
            "0.020405095653599017\n",
            "0.020311078053158578\n",
            "0.02021800409653164\n",
            "0.02012585710789958\n",
            "0.020034620779826998\n",
            "0.01994427916804346\n",
            "0.019854816685949378\n",
            "0.019766218098877814\n",
            "0.019678468518142943\n",
            "0.019591553394905776\n",
            "0.01950545851388616\n",
            "0.01942016998694885\n",
            "0.019335674246589933\n",
            "0.019251958039348403\n",
            "0.01916900841916599\n",
            "0.019086812740716254\n",
            "0.019005358652723327\n",
            "0.018924634091287692\n",
            "0.018844627273235753\n",
            "0.01876532668950804\n",
            "0.018686721098599385\n",
            "0.01860879952006318\n",
            "0.018531551228089905\n",
            "0.018454965745169676\n",
            "0.018379032835846982\n",
            "0.018303742500574063\n",
            "0.01822908496966958\n",
            "0.01815505069738745\n",
            "0.0180816303560996\n",
            "0.018008814830596534\n",
            "0.017936595212507997\n",
            "0.017864962794845913\n",
            "0.017793909066670614\n",
            "0.017723425707881612\n",
            "0.017653504584132774\n",
            "0.01758413774187229\n",
            "0.01751531740350607\n",
            "0.017447035962684775\n",
            "0.017379285979712472\n",
            "0.017312060177076125\n",
            "0.01724535143509351\n",
            "0.01717915278767856\n",
            "0.017113457418221498\n",
            "0.017048258655582068\n",
            "0.01698354997019318\n",
            "0.01691932497027289\n",
            "0.016855577398142325\n",
            "0.016792301126646998\n",
            "0.016729490155678966\n",
            "0.016667138608797476\n",
            "0.016605240729945386\n",
            "0.016543790880258907\n",
            "0.016482783534968393\n",
            "0.01642221328038713\n",
            "0.01636207481098629\n",
            "0.016302362926553418\n",
            "0.0162430725294316\n",
            "0.016184198621837825\n",
            "0.016125736303257726\n",
            "0.016067680767914337\n",
            "0.016010027302309226\n",
            "0.01595277128283336\n",
            "0.015895908173445956\n",
            "0.015839433523419374\n",
            "0.01578334296514746\n",
            "0.015727632212016632\n",
            "0.01567229705633657\n",
            "0.015617333367329863\n",
            "0.015562737089177991\n",
            "0.015508504239122691\n",
            "0.015454630905620619\n",
            "0.015401113246550118\n",
            "0.01534794748746832\n",
            "0.015295129919917137\n",
            "0.015242656899777007\n",
            "0.015190524845666891\n",
            "0.01513873023738891\n",
            "0.015087269614416934\n",
            "0.015036139574427435\n",
            "0.014985336771871678\n",
            "0.014934857916587828\n",
            "0.014884699772452228\n",
            "0.014834859156068456\n",
            "0.014785332935493209\n",
            "0.014736118028998187\n",
            "0.014687211403866486\n",
            "0.014638610075223355\n",
            "0.014590311104899457\n",
            "0.014542311600326614\n",
            "0.014494608713464226\n",
            "0.014447199639756643\n",
            "0.014400081617119434\n",
            "0.014353251924954754\n",
            "0.014306707883194474\n",
            "0.014260446851370422\n",
            "0.014214466227710979\n",
            "0.014168763448263432\n",
            "0.014123335986041125\n",
            "0.01407818135019506\n",
            "0.014033297085208653\n",
            "0.013988680770115962\n",
            "0.013944330017741625\n",
            "0.013900242473962716\n",
            "0.013856415816991492\n",
            "0.013812847756678478\n",
            "0.013769536033835576\n",
            "0.013726478419578133\n",
            "0.013683672714685818\n",
            "0.013641116748981486\n",
            "0.013598808380727868\n",
            "0.013556745496040966\n",
            "0.013514926008320343\n",
            "0.013473347857695193\n",
            "0.013432009010486015\n",
            "0.01339090745868158\n",
            "0.01335004121943015\n",
            "0.013309408334545213\n",
            "0.013269006870024877\n",
            "0.013228834915584261\n",
            "0.013188890584201265\n",
            "0.013149172011674471\n",
            "0.013109677356193164\n",
            "0.013070404797919403\n",
            "0.013031352538580811\n",
            "0.012992518801075104\n",
            "0.012953901829084466\n",
            "0.01291549988670085\n",
            "0.012877311258060896\n",
            "0.012839334246990436\n",
            "0.01280156717665861\n",
            "0.012764008389240853\n",
            "0.01272665624559041\n",
            "0.012689509124918568\n",
            "0.012652565424483191\n",
            "0.012615823559284523\n",
            "0.012579281961769438\n",
            "0.012542939081542616\n",
            "0.01250679338508488\n",
            "0.01247084335547874\n",
            "0.01243508749214036\n",
            "0.012399524310558198\n",
            "0.012364152342037887\n",
            "0.012328970133453066\n",
            "0.012293976247002332\n",
            "0.012259169259971761\n",
            "0.012224547764502929\n",
            "0.012190110367366148\n",
            "0.01215585568973924\n",
            "0.012121782366991065\n",
            "0.012087889048469917\n",
            "0.012054174397296525\n",
            "0.012020637090161906\n",
            "0.011987275817129417\n",
            "0.01195408928144111\n",
            "0.011921076199328254\n",
            "0.011888235299825958\n",
            "0.011855565324591515\n",
            "0.011823065027726717\n",
            "0.011790733175603823\n",
            "0.011758568546694966\n",
            "0.011726569931405093\n",
            "0.01169473613190839\n",
            "0.011663065961987682\n",
            "0.011631558246877428\n",
            "0.011600211823109325\n",
            "0.011569025538361258\n",
            "0.011537998251309009\n",
            "0.011507128831480919\n",
            "0.011476416159114974\n",
            "0.011445859125019177\n",
            "0.011415456630433899\n",
            "0.011385207586897072\n",
            "0.011355110916111955\n",
            "0.011325165549817093\n",
            "0.011295370429658651\n",
            "0.011265724507065186\n",
            "0.011236226743124404\n",
            "0.011206876108462521\n",
            "0.011177671583125057\n",
            "0.01114861215646044\n",
            "0.011119696827005083\n",
            "0.011090924602370609\n",
            "0.011062294499133199\n",
            "0.011033805542724373\n",
            "0.011005456767324017\n",
            "0.010977247215754955\n",
            "0.0109491759393794\n",
            "0.01092124199799695\n",
            "0.01089344445974444\n",
            "0.010865782400997318\n",
            "0.010838254906272595\n",
            "0.010810861068133459\n",
            "0.010783599987095258\n",
            "0.010756470771533095\n",
            "0.010729472537590861\n",
            "0.0107026044090917\n",
            "0.01067586551744977\n",
            "0.010649255001583547\n",
            "0.010622772007830307\n",
            "0.010596415689862016\n",
            "0.010570185208602513\n",
            "0.010544079732145873\n",
            "0.010518098435676046\n",
            "0.010492240501387667\n",
            "0.010466505118408248\n",
            "0.010440891482721155\n",
            "0.010415398797090157\n",
            "0.010390026270984691\n",
            "0.010364773120506564\n",
            "0.010339638568317431\n",
            "0.010314621843567512\n",
            "0.0102897221818254\n",
            "0.010264938825008611\n",
            "0.010240271021315438\n",
            "0.01021571802515756\n",
            "0.010191279097093671\n",
            "0.010166953503764184\n",
            "0.010142740517826581\n",
            "0.0101186394178919\n",
            "0.010094649488462098\n",
            "0.010070770019868171\n",
            "0.010047000308209106\n",
            "0.01002333965529195\n",
            "0.009999787368572328\n",
            "0.009976342761096095\n",
            "0.009953005151441515\n",
            "0.009929773863662538\n",
            "0.009906648227232447\n",
            "0.009883627576988728\n",
            "0.009860711253078223\n",
            "0.009837898600903366\n",
            "0.009815188971068967\n",
            "0.009792581719329712\n",
            "0.009770076206538395\n",
            "0.009747671798594837\n",
            "0.00972536786639545\n",
            "0.009703163785783425\n",
            "0.009681058937499704\n",
            "0.009659052707134396\n",
            "0.009637144485078996\n",
            "0.009615333666479046\n",
            "0.009593619651187662\n",
            "0.009572001843719381\n",
            "0.009550479653204633\n",
            "0.009529052493345075\n",
            "0.009507719782369105\n",
            "0.009486480942988169\n",
            "0.009465335402353612\n",
            "0.009444282592013926\n",
            "0.009423321947872624\n",
            "0.009402452910146752\n",
            "0.009381674923325638\n",
            "0.00936098743613044\n",
            "0.009340389901473919\n",
            "0.009319881776420957\n",
            "0.009299462522149332\n",
            "0.00927913160391123\n",
            "0.009258888490994956\n",
            "0.009238732656687195\n",
            "0.009218663578235807\n",
            "0.009198680736813047\n",
            "0.009178783617479005\n",
            "0.009158971709146016\n",
            "0.009139244504542706\n",
            "0.009119601500179245\n",
            "0.009100042196312432\n",
            "0.009080566096911517\n",
            "0.009061172709624324\n",
            "0.009041861545743678\n",
            "0.009022632120174475\n",
            "0.009003483951400746\n",
            "0.008984416561453457\n",
            "0.008965429475878466\n",
            "0.008946522223705046\n",
            "0.008927694337414449\n",
            "0.008908945352909203\n",
            "0.008890274809482387\n",
            "0.008871682249787578\n",
            "0.008853167219808958\n",
            "0.008834729268831736\n",
            "0.00881636794941301\n",
            "0.008798082817352861\n",
            "0.008779873431665851\n",
            "0.008761739354552727\n",
            "0.00874368015137254\n",
            "0.008725695390614978\n",
            "0.008707784643873112\n",
            "0.008689947485816321\n",
            "0.008672183494163661\n",
            "0.008654492249657298\n",
            "0.00863687333603647\n",
            "0.00861932634001153\n",
            "0.008601850851238433\n",
            "0.00858444646229332\n",
            "0.008567112768647597\n",
            "0.008549849368643005\n",
            "0.008532655863467273\n",
            "0.00851553185712969\n",
            "0.008498476956437235\n",
            "0.008481490770970664\n",
            "0.008464572913061153\n",
            "0.008447722997767023\n",
            "0.008430940642850602\n",
            "0.008414225468755537\n",
            "0.008397577098584264\n",
            "0.008380995158075598\n",
            "0.008364479275582787\n",
            "0.008348029082051529\n",
            "0.00833164421099839\n",
            "0.00831532429848936\n",
            "0.008299068983118744\n",
            "0.008282877905988046\n",
            "0.008266750710685257\n",
            "0.008250687043264326\n",
            "0.00823468655222477\n",
            "0.008218748888491516\n",
            "0.008202873705394945\n",
            "0.00818706065865114\n",
            "0.008171309406342335\n",
            "0.008155619608897574\n",
            "0.008139990929073514\n",
            "0.008124423031935364\n",
            "0.008108915584838255\n",
            "0.008093468257408473\n",
            "0.008078080721525093\n",
            "0.008062752651301717\n",
            "0.008047483723068391\n",
            "0.008032273615353674\n",
            "0.008017122008866989\n",
            "0.008002028586480971\n",
            "0.007986993033214169\n",
            "0.007972015036213745\n",
            "0.007957094284738484\n",
            "0.007942230470141778\n",
            "0.007927423285855037\n",
            "0.00791267242737104\n",
            "0.007897977592227528\n",
            "0.007883338479990926\n",
            "0.007868754792240239\n",
            "0.007854226232551158\n",
            "0.007839752506480073\n",
            "0.007825333321548653\n",
            "0.007810968387228103\n",
            "0.007796657414923986\n",
            "0.00778240011796086\n",
            "0.007768196211567205\n",
            "0.00775404541286057\n",
            "0.00773994744083267\n",
            "0.007725902016334758\n",
            "0.007711908862063042\n",
            "0.0076979677025443526\n",
            "0.007684078264121802\n",
            "0.007670240274940682\n",
            "0.007656453464934432\n",
            "0.0076427175658107865\n",
            "0.007629032311037944\n",
            "0.007615397435831042\n",
            "0.007601812677138468\n",
            "0.00758827777362872\n",
            "0.00757479246567689\n",
            "0.007561356495351696\n",
            "0.007547969606402359\n",
            "0.007534631544245647\n",
            "0.007521342055953214\n",
            "0.007508100890238818\n",
            "0.0074949077974457426\n",
            "0.007481762529534351\n",
            "0.007468664840069798\n",
            "0.007455614484209661\n",
            "0.007442611218691947\n",
            "0.0074296548018229725\n",
            "0.007416744993465458\n",
            "0.007403881555026781\n",
            "0.007391064249447146\n",
            "0.0073782928411881095\n",
            "0.007365567096220967\n",
            "0.007352886782015363\n",
            "0.0073402516675280536\n",
            "0.007327661523191618\n",
            "0.007315116120903374\n",
            "0.00730261523401444\n",
            "0.0072901586373187\n",
            "0.007277746107041959\n",
            "0.007265377420831409\n",
            "0.007253052357744716\n",
            "0.007240770698239741\n",
            "0.007228532224163909\n",
            "0.007216336718743836\n",
            "0.007204183966575198\n",
            "0.007192073753612412\n",
            "0.007180005867158562\n",
            "0.007167980095855377\n",
            "0.007155996229673358\n",
            "0.00714405405990184\n",
            "0.007132153379139318\n",
            "0.007120293981283677\n",
            "0.007108475661522667\n",
            "0.007096698216324361\n",
            "0.0070849614434277276\n",
            "0.007073265141833242\n",
            "0.007061609111793602\n",
            "0.007049993154804621\n",
            "0.007038417073595982\n",
            "0.007026880672122289\n",
            "0.007015383755554011\n",
            "0.007003926130268643\n",
            "0.006992507603841905\n",
            "0.006981127985038928\n",
            "0.006969787083805601\n",
            "0.0069584847112599945\n",
            "0.006947220679683842\n",
            "0.00693599480251401\n",
            "[[0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]]\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 1 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 1 1 0 1 0 0\n",
            " 1 0 1 0 0 1 1 1 0 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1\n",
            " 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 0 1\n",
            " 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 0 1 0\n",
            " 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 1\n",
            " 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 0 0 1 1\n",
            " 1 1 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0\n",
            " 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
            " 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 1\n",
            " 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1\n",
            " 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 1 0 0\n",
            " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 0 0 0 0 0 0 1]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}